{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "For recommender systems, we want to predict the rating of an item. This is very similar to a regression task in which we are trying to predict some sort continuous value. Therefore the evaluation metric will be similar aswell. \n",
    "\n",
    "For example, lets say we have a ratings matrix $R$ where $r_{uj}$ is the known rating that user *u* gave to item *j*. The recommender system predicts a value of the rating as $\\hat{r}_{uj}.$ The *entry-specific* error of the predicted rating is given by:\n",
    "\n",
    "$$e_{uj} = \\hat{r}_{uj} - r_{uj}$$\n",
    "\n",
    "The overall error of the recommender system is calculated by taking the average of the *entry-specific* errors in terms of absolute values or squared values.\n",
    "\n",
    "There are other recommender systems that do not predict ratings, instead they recommend the top *k* items. Such recommender systems have different methods of evaluating accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components of Accuracy Evaluation\n",
    "\n",
    "1. Designing the accuracy evaluation: We need to split our rating entries so that only some are used to train the model and the rest are used to evaluate.\n",
    "\n",
    "2. Accuracy metrics: Accuracy metrics are used to evaluate the predicted rating of user-item combinations, or the accuracy of the top *k* predicted items.\n",
    "\n",
    "    * Accuracy of estimating ratings: \n",
    "\n",
    "$$MSE = \\frac{\\sum_{(u,j)\\in E}e_{uj}^2}{|E|}$$\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{(u,j)\\in E}e_{uj}^2}{|E|}}$$\n",
    "\n",
    "    * Accuracy of esimating rankings: Depending on the nature, one could use rank-correlation measures, utility-based measures, or the recerve operating characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "\n",
    "There is a trade-off between accuracy and coverage. There are two types of coverage: user-space coverage and item-space coverage.\n",
    "\n",
    "Most of the times we are going to be dealing with sparse rating matrices. So it is hard to find similar users to aid with making predictions when there is very few mutually rated items with other users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty\n",
    "\n",
    "Novelty is the likelihood the user will be recommended an item they are unaware of or have not seen before. Being recommended items a user has never seen allows them to gain insight on some the things they like or don't like that they did not know before.\n",
    "\n",
    "The most natural way of measuring novelty would be through online methods, though there are offline approaches that can be implemented using timestamps of the rating.\n",
    "\n",
    "The idea of novelty is that novel systems are better at recommending things that you will like in the future instead of recommending things you will like right now. Therefore all ratings that were created after a certain time are removed from the training data. Additionally some items before the time are also removed. The model uses the remaining data to train.\n",
    "\n",
    "The removed items are used to evaluate the recommender syste. Each item rated before time t that is correctly recommended, the novelty score is penalized. Each item after time to that is correctly recommended, the novely score increases.\n",
    "\n",
    "Novelty measures the differential accuracy between future and past predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serendipity\n",
    "\n",
    "Serendipity measures how suprised a user will be with a successful recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity\n",
    "\n",
    "Given a list of recommended items, the recommended items should be as diverse as possible. This is because if you recommend a list of similar items and the user does not like the top choice, chances are they won't like the other choices aswell. Presenting different types of items often increases the chance that the user might select one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting Ratings for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-Out\n",
    "\n",
    "In Hold-Out method, a fraction of the ratings are hidden and the remaining ratings are used to train the model. The accuracy of predicting the hidden ratings is used to determine the overall accuracy of the model. This method prevents overfitting because we aren't using the hidden ratings to train the model. However, this approach underestimates the true accuracy because what if the hold-out data contains rating data that is higher than the training data? This would mean we would get predicted ratings that are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "In cross validation, the ratings data is divided into k equal folds. One of the k folds is used for testing and the remaining is used for training. This process is repeated k times until each fold has been used to test the model. The average accuracy across the k folds is the reported accuracy. This approach can closely estimate the true accuracy when the value of k is large.\n",
    "\n",
    "A special case would be where k is equal to the number of ratings in the data. All the entries are used for training except for one which will be used for testing. This is called \"Leave-one-out cross-validation\". Such an approach can closely approximate the accuracy, it is computationally expensive to train the model |S| times where S is the set of entries in the ratings matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics in Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Accuracy of Ratings Prediction\n",
    "\n",
    "Lets say we have a ratings matrix $R$ where $r_{uj}$ is the known rating that user *u* gave to item *j*. The recommender system predicts a value of the rating as $\\hat{r}_{uj}.$ The *entry-specific* error of the predicted rating is given by:\n",
    "\n",
    "$$e_{uj} = \\hat{r}_{uj} - r_{uj}$$\n",
    "\n",
    "The overall error of the recommender system is calculated by taking the average of the *entry-specific* errors in terms of absolute values or squared values.\n",
    "\n",
    "This error can be leveraged in various ways to compute the overall error over the set of entries on which testing was performed on.\n",
    "\n",
    "$$MSE = \\frac{\\sum_{(u,j)\\in E}e_{uj}^2}{|E|}$$\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{(u,j)\\in E}e_{uj}^2}{|E|}}$$\n",
    "\n",
    "The RMSE is often used instead of the MSE because RMSE is in the units of the ratings rather than the squared ratings like the MSE. The RMSE penalizes really large errors due to the squared term whereas MAE is another measurement that does not heavily penalize extremely wrong results.\n",
    "\n",
    "$$MAE = \\frac{\\sum_{(u,j)\\in E}|e_{uj}|}{|E|}$$\n",
    "\n",
    "There are also normalized versions of these metrics with values ranging from 0 to 1. They are easier to intepret.\n",
    "\n",
    "### RMSE versus MAE\n",
    "\n",
    "Depends what you want. RMSE penalizes large errors much more than MAE. If robustness of prediction across various ratings is very important, than RMSE is a suitable measurement. RMSE is not a true reflection of the average error because a few large errors can really skew the results. \n",
    "\n",
    "MAE is a better reflection of the accuracy when outliers aren't that important to consider.\n",
    "\n",
    "### Long Tail Impact\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking Via Correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking Via Utility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking via ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
